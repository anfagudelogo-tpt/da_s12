{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6ae659",
   "metadata": {},
   "source": [
    "# üöÄ Ejercicio: Construcci√≥n de un Pipeline de Datos con PostgreSQL y Looker Studio\n",
    "\n",
    "En este ejercicio vas a construir un pipeline de datos completo que conecte Python con PostgreSQL y Looker Studio. El objetivo es automatizar el flujo de datos: desde su extracci√≥n, limpieza y almacenamiento, hasta su visualizaci√≥n en un dashboard interactivo.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Objetivo\n",
    "\n",
    "1. Leer las credenciales necesarias para conectarse a una base de datos PostgreSQL.\n",
    "2. Establecer la conexi√≥n usando Python y la librer√≠a `SQLAlchemy`.\n",
    "3. Ejecutar una consulta SQL que obtenga datos desde una tabla de la base de datos.\n",
    "4. Realizar una limpieza b√°sica de los datos obtenidos.\n",
    "5. Guardar el DataFrame limpio como una nueva tabla en PostgreSQL.\n",
    "6. Crear un nuevo proyecto en **Looker Studio**.\n",
    "7. Conectar Looker Studio a la nueva tabla de la base de datos.\n",
    "8. Crear visualizaciones gr√°ficas interactivas con los datos procesados.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Instrucciones\n",
    "\n",
    "### 1. üìÇ Leer credenciales desde un archivo `.env` o diccionario\n",
    "- Incluye los siguientes datos:\n",
    "  - `host`\n",
    "  - `dbname`\n",
    "  - `user`\n",
    "  - `password`\n",
    "\n",
    "### 2. üß± Conectarte a la base de datos PostgreSQL\n",
    "- Usa `SQLAlchemy` o `psycopg2` para conectarte.\n",
    "\n",
    "### 3. üì§ Ejecutar un `SELECT` para extraer los datos de una tabla (por ejemplo: `ventas_originales`)\n",
    "\n",
    "### 4. üßº Limpieza de datos\n",
    "- Elimina filas con valores vac√≠os (`NaN`).\n",
    "- Corrige nombres o formatos si es necesario (por ejemplo, columnas de texto o fechas).\n",
    "- Elimina duplicados.\n",
    "\n",
    "### 5. üßæ Guardar los datos limpios\n",
    "- Crea una nueva tabla en la base de datos, por ejemplo: `ventas_limpias`.\n",
    "\n",
    "### 6. üßë‚Äçüíª Crear un proyecto en Looker Studio\n",
    "- Ve a [https://lookerstudio.google.com](https://lookerstudio.google.com).\n",
    "- Crea un nuevo reporte.\n",
    "- Agrega una fuente de datos tipo **PostgreSQL** con las mismas credenciales.\n",
    "- Selecciona la tabla `ventas_limpias`.\n",
    "\n",
    "### 7. üìà Crear visualizaciones\n",
    "- Agrega los siguientes elementos al dashboard:\n",
    "  - Gr√°fico de barras: ventas por ciudad.\n",
    "  - L√≠nea de tiempo: evoluci√≥n de ventas por fecha.\n",
    "  - Tabla con totales y filtros por regi√≥n o cliente.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Resultado esperado\n",
    "\n",
    "Al finalizar este ejercicio deber√≠as tener:\n",
    "\n",
    "- Un c√≥digo en Python que ejecuta un pipeline ETL con PostgreSQL.\n",
    "- Una nueva tabla limpia en la base de datos.\n",
    "- Un dashboard en Looker Studio conectado a esa tabla, con gr√°ficos din√°micos e interactivos.\n",
    "\n",
    "---\n",
    "\n",
    "üí° *Este flujo es el que muchas empresas usan en el mundo real para tomar decisiones basadas en datos.* ¬°T√∫ tambi√©n puedes hacerlo!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e8d4b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from utils import (\n",
    "    conectar_postgresql,\n",
    "    ejecutar_query,\n",
    "    cargar_dataframe_postgresql,\n",
    "    read_json_file,\n",
    "    limpieza_general\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e51c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "credenciales = read_json_file('credentials.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e87d1e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conexi√≥n exitosa a PostgreSQL.\n"
     ]
    }
   ],
   "source": [
    "conn = conectar_postgresql(credenciales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205e4550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Consulta ejecutada exitosamente.\n",
      "‚úÖ Consulta ejecutada exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andres Agudelo\\Documents\\da_s12\\class_1\\utils.py:51: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    }
   ],
   "source": [
    "query = 'SELECT * FROM datos_transaccionales'\n",
    "# query_2 = 'SELECT * FROM film'\n",
    "\n",
    "# film = ejecutar_query(conn, query_2 )\n",
    "df = ejecutar_query(conn, query )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12c04293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Columna 'sale_date' convertida a fecha.\n",
      "\n",
      "üìä Tipos de datos:\n",
      "sale_id                 int64\n",
      "customer_id             int64\n",
      "category               object\n",
      "amount                  int64\n",
      "sale_date      datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "üîç Valores nulos por columna:\n",
      "sale_id        0\n",
      "customer_id    0\n",
      "category       0\n",
      "amount         0\n",
      "sale_date      0\n",
      "dtype: int64\n",
      "\n",
      "‚úÖ Limpieza general completada.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andres Agudelo\\Documents\\da_s12\\class_1\\utils.py:145: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors=\"raise\")\n"
     ]
    }
   ],
   "source": [
    "df_clean = limpieza_general(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7db162de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tabla 'datos_transaccionales_limpios' creada o reemplazada exitosamente.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cargar_dataframe_postgresql(\n",
    "    df_clean,\n",
    "    'datos_transaccionales_limpios',\n",
    "    credenciales\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipeline_etl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
